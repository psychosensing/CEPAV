{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d4836d-9e40-4019-ae1c-299512d8172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moments - S3:\n",
    "# Baseline 1,2,3,4,5 minute (Marker name: 201), \n",
    "# Before the 1st match 1st minute (211), \n",
    "# Before the 1st match 2nd minute (212), \n",
    "# During the match 1st minute (213), \n",
    "# During the match 2nd minute (214), \n",
    "# After the 1st match 1st minute (215), \n",
    "# After the 1st match 2nd minute, (215)\n",
    "# and similarly for the 2nd to 8th matches -   \n",
    "# a total of 53 one-minute intervals from S3 (5 from the baseline and 6 from each match)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39e39b1f-ae65-4f02-a865-349b55d4a7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neurokit2 as nk\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "574df94d-4c5d-4318-b0d8-1ba13a81fdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_channel(cols_template, channel):\n",
    "    '''\n",
    "    Function to modify a template of column names to adapt to each signal.\n",
    "\n",
    "    Input:\n",
    "    cols_template (list of str): A list of column names containing a placeholder \"[channel]\".\n",
    "    channel (str): The name of the channel to replace the placeholder.\n",
    "\n",
    "    Returns:\n",
    "    list of str: A list of column names with the \"[channel]\" placeholder replaced by the provided channel name.\n",
    "    '''\n",
    "    \n",
    "    return [re.sub(r\"\\[channel\\]\", channel, col) for col in cols_template]\n",
    "\n",
    "def process_hr_hrv_s2(file_path, bunch_of_cols, filename):\n",
    "     '''\n",
    "    Function to process heart rate (HR) and heart rate variability (HRV) data from ECG signals in a CSV file.\n",
    "\n",
    "    Input:\n",
    "    file_path (str): The path to the CSV file containing the ECG data.\n",
    "    bunch_of_cols (list of lists of str): A list of lists where each sublist contains column names for HR and HRV data.\n",
    "    filename (str): The name of the file to be used as the index for the resulting DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with processed HR and HRV data. The index is based on the provided filename,\n",
    "                  and the columns are defined by the input `bunch_of_cols`.\n",
    "    '''\n",
    "    i = 0\n",
    "    bn_col = 0\n",
    "    df_sub = pd.read_csv(file_path)\n",
    "    markers = [201,211,212,213,214,215,216,221,222,223,224,225,226,231,232,233,234,235,236,241,242,243,244,245,246,251,252,253,254,255,256,261,262,263,264,265,266,271,272,273,274,275,276,281,282,\n",
    "               283,284,285,286] #procedure markers, needed to mark the start of the average counting moment for each interval.\n",
    "                                #for marker \"201\" there are 5 columns from each set with the label \"baseline\". For the next markers there is a single column.\n",
    "    dataframes = []\n",
    "    hr_hrv_cols = [col for sublist in bunch_of_cols[:2] for col in sublist]\n",
    "    df_hr = pd.DataFrame(columns=hr_hrv_cols)\n",
    "    df_hr.loc[0] = [None] * len(hr_hrv_cols)\n",
    "    \n",
    "    if bn_col == 0:\n",
    "        for marker in markers:\n",
    "            curr_idx = df_sub.index[df_sub['marker'] == marker] #determining the first index for the given marker in df\n",
    "            \n",
    "            if marker == 201:\n",
    "                \n",
    "                for y in range(0, 300000, 60000): #calculations initiated from a 5-minute baseline, with minute intervals\n",
    "                    ecg_signals, info = nk.ecg_process(df_sub[\"ECG\"].iloc[curr_idx[0]+y:curr_idx[0]+y+60000], sampling_rate=1000) #calculating minute intervals hr from the first found index based on the marker, with a sampling rate of 1000hz\n",
    "                    hr = nk.ecg_intervalrelated(ecg_signals).iloc[0, 0] \n",
    "                    df_hr.iloc[0, i] = hr #saving calculations in local df\n",
    "                    i += 1 #moving to the next column\n",
    "            else:\n",
    "                ecg_signals, info = nk.ecg_process(df_sub[\"ECG\"].iloc[curr_idx[0]-1:curr_idx[0]+60000], sampling_rate=1000)\n",
    "                hr = nk.ecg_intervalrelated(ecg_signals).iloc[0, 0]\n",
    "                df_hr.iloc[0, i] = hr\n",
    "                i += 1\n",
    "        bn_col += 1\n",
    "    \n",
    "    if bn_col == 1:\n",
    "        for marker in markers:\n",
    "            curr_idx = df_sub.index[df_sub['marker'] == marker]\n",
    "            if marker == 201:\n",
    "                for y in range(0, 300000, 60000): #similarly for hrv\n",
    "                    ecg_signals, info = nk.ecg_process(df_sub[\"ECG\"].iloc[curr_idx[0]+y:curr_idx[0]+y+60000], sampling_rate=1000)\n",
    "                    hrv = nk.ecg_intervalrelated(ecg_signals).iloc[0, 9]\n",
    "                    df_hr.iloc[0, i] = hrv\n",
    "                    i += 1\n",
    "            else:\n",
    "                ecg_signals, info = nk.ecg_process(df_sub[\"ECG\"].iloc[curr_idx[0]:curr_idx[0]+60000], sampling_rate=1000)\n",
    "                hrv = nk.ecg_intervalrelated(ecg_signals).iloc[0, 9]\n",
    "                df_hr.iloc[0, i] = hrv\n",
    "                i += 1\n",
    "    \n",
    "    df_hr.index = [filename.replace('.csv', '')] #saving to df with the subject's id as the index name\n",
    "    return df_hr\n",
    "\n",
    "def process_others_s2(df, file_path, bunch_of_cols, filename):\n",
    "    '''\n",
    "    Function to process blood pressure channels together with accelerometers data and merge with the existing DataFrame.\n",
    "\n",
    "    Input:\n",
    "    df (pd.DataFrame): The existing DataFrame with calculated HR and HRV means to merge new data into.\n",
    "    file_path (str): The path to the CSV file containing the data.\n",
    "    bunch_of_cols (list of lists of str): A list of lists where each sublist contains column names for the different physiological metrics.\n",
    "    filename (str): The name of the file to be used as the index for the resulting DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with the processed data merged into the input DataFrame `df`.\n",
    "                  The index is based on the provided filename, and the columns are defined by the input `bunch_of_cols`.\n",
    "    '''\n",
    "    i = 0\n",
    "    bn_col = 0\n",
    "    df_sub = pd.read_csv(file_path, usecols=['SBP', 'DBP', 'CO', 'TPR', 'wr', 'tl', 'tr', 'marker'])\n",
    "    markers = [201,211,212,213,214,215,216,221,222,223,224,225,226,231,232,233,234,235,236,241,242,243,244,245,246,251,252,253,254,255,256,261,262,263,264,265,266,271,272,273,274,275,276,281,282,\n",
    "               283,284,285,286]\n",
    "    dataframes = []\n",
    "    for set_cols in bunch_of_cols[2:]: #selecting sets of columns excluding hr and hrv\n",
    "        df_temp = pd.DataFrame(columns=set_cols)\n",
    "        df_temp.loc[0] = [None] * len(set_cols)\n",
    "        for marker in markers:\n",
    "            curr_idx = df_sub.index[df_sub['marker'] == marker]\n",
    "            if marker == 201:\n",
    "                for y in range(0, 300000, 60000):\n",
    "                    df_sub_means = df_sub.iloc[curr_idx[0]+y:curr_idx[0]+60000+y, :-1].mean().iloc[bn_col] #calculating the average for minute fragments of the baseline\n",
    "                    df_temp.iloc[0, i] = df_sub_means #incorporating data into df in the correct place\n",
    "                    i += 1 #moving to the next column \n",
    "            else:\n",
    "                \n",
    "                # print(df_sub.iloc[curr_idx[0]+y:curr_idx[0]+60000+y, :-1].mean())\n",
    "                df_sub_means = df_sub.iloc[curr_idx[0]:curr_idx[0]+60000, :-1].mean().iloc[bn_col]\n",
    "                df_temp.iloc[0, i] = df_sub_means\n",
    "                i += 1\n",
    "        dataframes.append(df_temp) #collecting dataframes into a list\n",
    "        bn_col += 1\n",
    "        i = 0\n",
    "    \n",
    "    df_final = pd.concat(dataframes, axis=1) #merging df with calculated hr and hrv with the list of calculated dfs from the averages of blood pressure and accelerometers\n",
    "    df_final.index = [filename.replace('.csv', '')]\n",
    "    \n",
    "   \n",
    "    \n",
    "    df_merged = df.join(df_final, how='outer')\n",
    "    return df_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cc64407-d4e9-4f7e-9d6c-71f907605f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['ECG_Rate_mean', 'HRV_RMSSD', 'SBP', 'DBP', 'CO', 'TPR', 'wr', 'tl', 'tr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c67d71f7-abea-4155-bf3b-bedf891afb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_channels = ['HR', 'HRV', 'SBP', 'DBP', 'CO', 'TPR', 'wr', 'tl' ,'tr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e332b48-cd8a-4b3e-a2f1-7b50c37a6c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the column template for channels \n",
    "cols_template = [\"baseline_visit2_min1_[channel]\", \"baseline_visit2_min2_[channel]\", \"baseline_visit2_min3_[channel]\", \"baseline_visit2_min4_[channel]\", \"baseline_visit2_min5_[channel]\", \"tournament1_baseline_min1_[channel]\",\n",
    "\"tournament1_baseline_min2_[channel]\", \"tournament1_gameplay_min1_[channel]\", \"tournament1_gameplay_min2_[channel]\", \"tournament1_recovery_min1_[channel]\", \"tournament1_recovery_min2_[channel]\", \"tournament2_baseline_min1_[channel]\",\n",
    "\"tournament2_baseline_min2_[channel]\", \"tournament2_gameplay_min1_[channel]\", \"tournament2_gameplay_min2_[channel]\", \"tournament2_recovery_min1_[channel]\", \"tournament2_recovery_min2_[channel]\", \"tournament3_baseline_min1_[channel]\",\n",
    "\"tournament3_baseline_min2_[channel]\", \"tournament3_gameplay_min1_[channel]\", \"tournament3_gameplay_min2_[channel]\", \"tournament3_recovery_min1_[channel]\", \"tournament3_recovery_min2_[channel]\", \"tournament4_baseline_min1_[channel]\",\n",
    "\"tournament4_baseline_min2_[channel]\", \"tournament4_gameplay_min1_[channel]\", \"tournament4_gameplay_min2_[channel]\", \"tournament4_recovery_min1_[channel]\", \"tournament4_recovery_min2_[channel]\", \"tournament5_baseline_min1_[channel]\",\n",
    "\"tournament5_baseline_min2_[channel]\", \"tournament5_gameplay_min1_[channel]\", \"tournament5_gameplay_min2_[channel]\", \"tournament5_recovery_min1_[channel]\", \"tournament5_recovery_min2_[channel]\", \"tournament6_baseline_min1_[channel]\",\n",
    "\"tournament6_baseline_min2_[channel]\", \"tournament6_gameplay_min1_[channel]\", \"tournament6_gameplay_min2_[channel]\", \"tournament6_recovery_min1_[channel]\", \"tournament6_recovery_min2_[channel]\", \"tournament7_baseline_min1_[channel]\",\n",
    "\"tournament7_baseline_min2_[channel]\", \"tournament7_gameplay_min1_[channel]\", \"tournament7_gameplay_min2_[channel]\", \"tournament7_recovery_min1_[channel]\", \"tournament7_recovery_min2_[channel]\", \"tournament8_baseline_min1_[channel]\",\n",
    "\"tournament8_baseline_min2_[channel]\", \"tournament8_gameplay_min1_[channel]\", \"tournament8_gameplay_min2_[channel]\", \"tournament8_recovery_min1_[channel]\", \"tournament8_recovery_min2_[channel]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c63a7132-453a-438d-a473-7c623cf545e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_cols = replace_channel(cols_template, list_of_channels[0])\n",
    "hrv_cols = replace_channel(cols_template, list_of_channels[1])\n",
    "sbp_cols = replace_channel(cols_template, list_of_channels[2])\n",
    "dbp_cols = replace_channel(cols_template, list_of_channels[3])\n",
    "co_cols = replace_channel(cols_template, list_of_channels[4])\n",
    "tpr_cols = replace_channel(cols_template, list_of_channels[5])\n",
    "wr_cols = replace_channel(cols_template, list_of_channels[6])\n",
    "tl_cols = replace_channel(cols_template, list_of_channels[7])\n",
    "tr_cols = replace_channel(cols_template, list_of_channels[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adc9034f-32de-497d-a4ee-f6ade84aed30",
   "metadata": {},
   "outputs": [],
   "source": [
    "bunch = [hr_cols, hrv_cols, sbp_cols, dbp_cols, co_cols, tpr_cols, wr_cols, tl_cols, tr_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6e7f952-13d6-4418-9f9a-56369006e67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b491df57-d403-4b15-be6e-dbc1a1d6b39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S2_p281.csv\n",
      "S2_p17.csv\n",
      "S2_p226.csv\n",
      "S2_p110.csv\n",
      "S2_p53.csv\n",
      "S2_p192.csv\n",
      "S2_p21.csv\n",
      "S2_p175.csv\n",
      "S2_p129.csv\n",
      "S2_p45.csv\n",
      "S2_p299.csv\n",
      "S2_p215.csv\n",
      "S2_p207.csv\n",
      "S2_p180.csv\n",
      "S2_p252.csv\n",
      "S2_p263.csv\n",
      "S2_p199.csv\n",
      "S2_p154.csv\n",
      "S2_p217.csv\n",
      "S2_p242.csv\n",
      "S2_p107.csv\n",
      "S2_p169.csv\n",
      "S2_p298.csv\n",
      "S2_p188.csv\n",
      "S2_p34.csv\n",
      "S2_p1.csv\n",
      "S2_p114.csv\n",
      "S2_p294.csv\n",
      "S2_p190.csv\n",
      "S2_p291.csv\n",
      "S2_p138.csv\n",
      "S2_p60.csv\n",
      "S2_p232.csv\n",
      "S2_p258.csv\n",
      "S2_p153.csv\n",
      "S2_p255.csv\n",
      "S2_p233.csv\n",
      "S2_p295.csv\n",
      "S2_p218.csv\n",
      "S2_p117.csv\n",
      "S2_p282.csv\n",
      "S2_p278.csv\n",
      "S2_p5.csv\n",
      "S2_p93.csv\n",
      "S2_p225.csv\n",
      "S2_p83.csv\n",
      "S2_p24.csv\n",
      "S2_p267.csv\n",
      "S2_p155.csv\n",
      "S2_p201.csv\n",
      "S2_p41.csv\n",
      "S2_p134.csv\n",
      "S2_p300.csv\n",
      "S2_p146.csv\n",
      "S2_p51.csv\n",
      "S2_p81.csv\n",
      "S2_p116.csv\n",
      "S2_p147.csv\n",
      "S2_p29.csv\n",
      "S2_p160.csv\n",
      "S2_p203.csv\n",
      "S2_p66.csv\n",
      "S2_p256.csv\n",
      "S2_p230.csv\n",
      "S2_p148.csv\n",
      "S2_p174.csv\n",
      "S2_p183.csv\n",
      "S2_p196.csv\n",
      "S2_p219.csv\n",
      "S2_p32.csv\n",
      "S2_p54.csv\n",
      "S2_p261.csv\n",
      "S2_p168.csv\n",
      "S2_p44.csv\n",
      "S2_p16.csv\n",
      "S2_p46.csv\n",
      "S2_p131.csv\n",
      "S2_p202.csv\n",
      "S2_p142.csv\n",
      "S2_p251.csv\n",
      "S2_p77.csv\n",
      "S2_p189.csv\n",
      "S2_p97.csv\n",
      "S2_p22.csv\n",
      "S2_p40.csv\n",
      "S2_p86.csv\n",
      "S2_p265.csv\n",
      "S2_p167.csv\n",
      "S2_p89.csv\n",
      "S2_p238.csv\n",
      "S2_p43.csv\n",
      "S2_p151.csv\n",
      "S2_p3.csv\n",
      "S2_p187.csv\n",
      "S2_p280.csv\n",
      "S2_p130.csv\n",
      "S2_p106.csv\n",
      "S2_p61.csv\n",
      "S2_p145.csv\n",
      "S2_p133.csv\n",
      "S2_p2.csv\n",
      "S2_p296.csv\n",
      "S2_p271.csv\n",
      "S2_p287.csv\n",
      "S2_p239.csv\n",
      "S2_p71.csv\n",
      "S2_p177.csv\n",
      "S2_p115.csv\n",
      "S2_p246.csv\n",
      "S2_p229.csv\n",
      "S2_p240.csv\n",
      "S2_p185.csv\n",
      "S2_p173.csv\n",
      "S2_p6.csv\n",
      "S2_p209.csv\n",
      "S2_p111.csv\n",
      "S2_p231.csv\n",
      "S2_p272.csv\n",
      "S2_p264.csv\n",
      "S2_p253.csv\n",
      "S2_p254.csv\n",
      "S2_p157.csv\n",
      "S2_p118.csv\n",
      "S2_p38.csv\n",
      "S2_p214.csv\n",
      "S2_p55.csv\n",
      "S2_p144.csv\n",
      "S2_p191.csv\n",
      "S2_p57.csv\n",
      "S2_p90.csv\n",
      "S2_p262.csv\n",
      "S2_p165.csv\n",
      "S2_p235.csv\n",
      "S2_p72.csv\n",
      "S2_p70.csv\n",
      "S2_p56.csv\n",
      "S2_p178.csv\n",
      "S2_p297.csv\n",
      "S2_p211.csv\n",
      "S2_p63.csv\n",
      "S2_p101.csv\n",
      "S2_p248.csv\n",
      "S2_p249.csv\n",
      "S2_p94.csv\n",
      "S2_p39.csv\n",
      "S2_p49.csv\n",
      "S2_p98.csv\n",
      "S2_p84.csv\n",
      "S2_p112.csv\n",
      "S2_p170.csv\n",
      "S2_p80.csv\n",
      "S2_p266.csv\n",
      "S2_p42.csv\n",
      "S2_p221.csv\n",
      "S2_p227.csv\n",
      "S2_p268.csv\n",
      "S2_p27.csv\n",
      "S2_p186.csv\n",
      "S2_p23.csv\n",
      "S2_p103.csv\n",
      "S2_p35.csv\n",
      "S2_p26.csv\n",
      "S2_p260.csv\n",
      "S2_p181.csv\n",
      "S2_p104.csv\n",
      "S2_p156.csv\n",
      "S2_p79.csv\n",
      "S2_p269.csv\n",
      "S2_p141.csv\n",
      "S2_p36.csv\n",
      "S2_p213.csv\n",
      "S2_p96.csv\n",
      "S2_p241.csv\n",
      "S2_p75.csv\n",
      "S2_p159.csv\n",
      "S2_p48.csv\n",
      "S2_p171.csv\n",
      "S2_p200.csv\n",
      "S2_p152.csv\n",
      "S2_p137.csv\n",
      "S2_p143.csv\n",
      "S2_p220.csv\n",
      "S2_p212.csv\n",
      "S2_p128.csv\n",
      "S2_p28.csv\n",
      "S2_p237.csv\n",
      "S2_p19.csv\n",
      "S2_p276.csv\n",
      "S2_p100.csv\n",
      "S2_p279.csv\n",
      "S2_p7.csv\n",
      "S2_p113.csv\n",
      "S2_p95.csv\n",
      "S2_p205.csv\n",
      "S2_p109.csv\n",
      "S2_p68.csv\n",
      "S2_p197.csv\n",
      "S2_p69.csv\n",
      "S2_p245.csv\n",
      "S2_p52.csv\n",
      "S2_p76.csv\n",
      "S2_p74.csv\n",
      "S2_p162.csv\n",
      "S2_p47.csv\n",
      "S2_p228.csv\n",
      "S2_p149.csv\n",
      "S2_p257.csv\n",
      "S2_p224.csv\n",
      "S2_p33.csv\n",
      "S2_p139.csv\n",
      "S2_p11.csv\n",
      "S2_p286.csv\n",
      "S2_p122.csv\n",
      "S2_p123.csv\n",
      "S2_p30.csv\n",
      "S2_p127.csv\n",
      "S2_p18.csv\n",
      "S2_p119.csv\n",
      "S2_p247.csv\n",
      "S2_p124.csv\n",
      "S2_p15.csv\n",
      "S2_p285.csv\n",
      "S2_p222.csv\n",
      "S2_p25.csv\n",
      "S2_p136.csv\n",
      "S2_p108.csv\n",
      "S2_p194.csv\n",
      "S2_p14.csv\n",
      "S2_p166.csv\n",
      "S2_p102.csv\n",
      "S2_p10.csv\n",
      "S2_p73.csv\n",
      "S2_p250.csv\n",
      "S2_p259.csv\n",
      "S2_p12.csv\n",
      "S2_p179.csv\n",
      "S2_p13.csv\n",
      "S2_p20.csv\n",
      "S2_p283.csv\n",
      "S2_p293.csv\n",
      "S2_p223.csv\n",
      "S2_p216.csv\n",
      "S2_p135.csv\n",
      "S2_p270.csv\n",
      "S2_p277.csv\n",
      "S2_p92.csv\n",
      "S2_p59.csv\n",
      "S2_p64.csv\n",
      "S2_p58.csv\n",
      "S2_p91.csv\n",
      "S2_p236.csv\n",
      "S2_p99.csv\n",
      "S2_p85.csv\n",
      "S2_p273.csv\n",
      "S2_p206.csv\n",
      "S2_p87.csv\n",
      "S2_p78.csv\n",
      "S2_p195.csv\n",
      "S2_p161.csv\n",
      "S2_p163.csv\n",
      "S2_p292.csv\n",
      "S2_p121.csv\n",
      "S2_p210.csv\n",
      "S2_p65.csv\n",
      "S2_p243.csv\n",
      "S2_p172.csv\n",
      "S2_p208.csv\n",
      "S2_p204.csv\n",
      "S2_p105.csv\n",
      "S2_p132.csv\n",
      "S2_p176.csv\n",
      "S2_p284.csv\n",
      "S2_p120.csv\n",
      "S2_p82.csv\n",
      "S2_p150.csv\n",
      "S2_p88.csv\n",
      "S2_p62.csv\n",
      "S2_p158.csv\n",
      "S2_p288.csv\n",
      "S2_p67.csv\n",
      "S2_p50.csv\n",
      "S2_p4.csv\n",
      "S2_p193.csv\n",
      "S2_p234.csv\n",
      "S2_p140.csv\n",
      "S2_p31.csv\n",
      "S2_p125.csv\n",
      "S2_p37.csv\n",
      "S2_p244.csv\n",
      "S2_p184.csv\n",
      "S2_p8.csv\n",
      "S2_p198.csv\n",
      "S2_p126.csv\n"
     ]
    }
   ],
   "source": [
    "# Processing all CSV files in the folder\n",
    "input_folder = '/home/ubuntu/eSportData/jupyter-data/VU_AMS/s2_output/'\n",
    "output_df = pd.DataFrame()\n",
    "\n",
    "for file_name in os.listdir(input_folder):  #iterating through the list with the databases of the subjects\n",
    "    if file_name.endswith('.csv'):\n",
    "        try:\n",
    "            print(file_name)\n",
    "            file_path = os.path.join(input_folder, file_name)\n",
    "            processed_df = process_hr_hrv_s2(file_path, bunch, file_name) #calling the function to create df with calculated hr and hrv averages\n",
    "            final_df = process_others_s2(processed_df, file_path, bunch, file_name) #calling the function to calculate blood pressure and accelerometer averages and merge them with hr and hrv into a common df\n",
    "            output_df = pd.concat([output_df, final_df]) #adding the subject's data from the iteration to the collective df with other subjects\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Saving the resulting DataFrame to a CSV file\n",
    "output_df.to_csv('/home/ubuntu/eSportData/jupyter-data/VU_AMS/processed_means_s2_v2_patched.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
