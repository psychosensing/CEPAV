{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c44dd97e-e83f-4161-b537-f1c0d09e495e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import datetime, time\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "path = '/home/ubuntu/eSportData/jupyter-data/VU_AMS/s1'\n",
    "path_out = '/home/ubuntu/eSportData/jupyter-data/VU_AMS/samples'\n",
    "path_tx = '/home/ubuntu/eSportData/jupyter-data/beatscope/Pliki/S1'\n",
    "path_acc = '/home/ubuntu/eSportData/jupyter-data/akcelerometry/S1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06f87cb2-8e70-4f46-8b89-addbd2663648",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rejected = []\n",
    "output_filename = \"output.txt\"\n",
    "rejected_list = 'rejected.txt'\n",
    "outputs = []\n",
    "output_errors_path = '/home/ubuntu/eSportData/jupyter-data/VU_AMS/error_s1.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6bb3bf8e-352d-44c5-b183-0c1368f3445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_time_from_file(filename):\n",
    "    \"\"\"\n",
    "    function to read time from vuams files\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            next(file)\n",
    "            second_row = next(file).strip()\n",
    "            parts = second_row.split(\"/\")\n",
    "            return str(parts[1])\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File '{filename}' not found.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def merge_vs_tx(start_markers, vuams, beat, columns, da_df, tls, keys):\n",
    "    \"\"\"\n",
    "    function to merge vuams df with beatscope files. All files have the same number of rows after adding the time column from vuams to each beatscope file.\n",
    "    input : \n",
    "        start_markers: information about indexes where vuams markers start for the whole procedure\n",
    "        vuams: vuams df with the first measurement merged with beatscope (to be able to freely iterate over the whole set of needed columns)\n",
    "        columns: vuams and beatscope columns needed to assemble the dataframe\n",
    "        da_df: vuams df before merging with the first beatscope\n",
    "        tls: list with all beatscope dfs\n",
    "        keys: dictionary of markers, keys -> vuams markers, values -> beatscope markers. All as strings\n",
    "    output:\n",
    "        merged vuams df with all beatscope files\n",
    "    \"\"\"\n",
    "    for markers in start_markers[1:]:  # skip the first marker because the first beatscope file is already added to vuams using a simple merge\n",
    "        marker_vms_dict = {1:0, 15:1, 21:2, 25:3, 35:4} # marker and corresponding beatscope index from the list of all\n",
    "        keys_idx = marker_vms_dict.keys()\n",
    "        da_sync_value = da_df.loc[markers, 'marker'] # and vice versa - marker value corresponding to the index\n",
    "        da_sync_value = int(da_sync_value)\n",
    "        da_sync_value = str(da_sync_value)\n",
    "        print(da_sync_value, 'da sync')\n",
    "        which_index = start_markers.index(markers) # information about iteration\n",
    "        # try:\n",
    "        tx_sync_idx_m1 = tls[which_index].index[tls[which_index]['marker_tx'] == keys[da_sync_value]][0] # find in the given beatscope from the list of beatscopes (tls) the index where the marker starts (m10, m11, etc.)\n",
    "        print(tx_sync_idx_m1)\n",
    "        rows_diff = tx_sync_idx_m1 - markers # row difference between beatscope and vuams\n",
    "        print(rows_diff)\n",
    "\n",
    "        for row in range(markers,len(vuams)): # iterate only from the index where the vuams marker starts\n",
    "                for value_idx in range(len(vuams[row])):\n",
    "                     \n",
    "                    if vuams[markers][value_idx] in keys_idx: #in keys: # add to specific vuams columns data from beatscope columns considering row differences in dataframes\n",
    "                        match = vuams[markers][value_idx]\n",
    "                        try:\n",
    "                                vuams[row][7] = beat[marker_vms_dict[match]][row+rows_diff][1]\n",
    "                                vuams[row][8] = beat[marker_vms_dict[match]][row+rows_diff][2]\n",
    "                                vuams[row][9] = beat[marker_vms_dict[match]][row+rows_diff][3]\n",
    "                                vuams[row][10] = beat[marker_vms_dict[match]][row+rows_diff][4]\n",
    "                                vuams[row][11] = beat[marker_vms_dict[match]][row+rows_diff][5]\n",
    "                                vuams[row][12] = beat[marker_vms_dict[match]][row+rows_diff][6]\n",
    "                                vuams[row][13] = beat[marker_vms_dict[match]][row+rows_diff][7]\n",
    "                                vuams[row][14] = beat[marker_vms_dict[match]][row+rows_diff][8]\n",
    "                                vuams[row][15] = beat[marker_vms_dict[match]][row+rows_diff][9]\n",
    "                                vuams[row][16] = beat[marker_vms_dict[match]][row+rows_diff][10]\n",
    "                                vuams[row][17] = beat[marker_vms_dict[match]][row+rows_diff][11]\n",
    "                                vuams[row][-2] = beat[marker_vms_dict[match]][row+rows_diff][12]\n",
    "                        except:\n",
    "                                pass\n",
    "                  \n",
    "    merged_df = pd.DataFrame(vuams, columns =columns) # dataframe with merged information from beatscope\n",
    "    return merged_df\n",
    "def execute_and_store_output(filename, func, *args, **kwargs):\n",
    "    try:\n",
    "        sys.stdout = open(filename, 'w')\n",
    "        func(*args, **kwargs)\n",
    "    finally:\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = sys.__stdout__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481705f2-7c53-4f84-a57a-37f7b6e7b5f0",
   "metadata": {},
   "source": [
    "S1_p123 - -9999 dac na kanałach 2,3,4\n",
    "S1_p236 - -9999 dac na kanałach 1\n",
    "S1_p245 - -9999 dac na kanałach 2,3,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4208ff3-4ef3-4830-b45d-0e328d4d3cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f806fcb-30e6-4af0-91c7-8cbe7b8847de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232 -numer osoby\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2862483/608676873.py:108: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_dz = pd.concat([df_dz, empty_rows_dz], ignore_index=True)\n",
      "/tmp/ipykernel_2862483/608676873.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_z0 = pd.concat([df_z0, empty_rows_z0], ignore_index=True)\n",
      "/tmp/ipykernel_2862483/608676873.py:113: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_dzdt = pd.concat([df_dzdt, empty_rows_dzdt], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:55:50.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2862483/608676873.py:120: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_ecg = pd.concat([df_ecg, empty_rows_ecg], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check\n",
      "1900-01-01 09:09:15.141000\n",
      "1900-01-01 09:23:20.068000\n",
      "1900-01-01 10:23:04.338000\n",
      "1900-01-01 10:29:21.470000\n",
      "1900-01-01 10:37:45.296000\n",
      "check2\n",
      "5\n",
      "5 długość start markers\n",
      "15 hjeolo\n",
      "910740\n",
      "51186\n",
      "21 hjeolo\n",
      "4486732\n",
      "51734\n",
      "25 hjeolo\n",
      "4862846\n",
      "52029\n",
      "35 hjeolo\n",
      "5368022\n",
      "51662\n",
      "check 3\n",
      "/home/ubuntu/eSportData/jupyter-data/akcelerometry/S1/S1_232.csv acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2862483/608676873.py:248: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  acc['time'] = pd.to_datetime(acc['time'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:40:24.001000\n",
      "10:41:29.285000\n",
      "main path\n",
      "0.0\n",
      "nan\n",
      "check4\n",
      "done\n",
      "235 -numer osoby\n",
      "16:41:26.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2862483/608676873.py:120: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_ecg = pd.concat([df_ecg, empty_rows_ecg], ignore_index=True)\n",
      "/tmp/ipykernel_2862483/608676873.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_z0 = pd.concat([df_z0, empty_rows_z0], ignore_index=True)\n",
      "/tmp/ipykernel_2862483/608676873.py:108: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_dz = pd.concat([df_dz, empty_rows_dz], ignore_index=True)\n",
      "/tmp/ipykernel_2862483/608676873.py:113: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_dzdt = pd.concat([df_dzdt, empty_rows_dzdt], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check\n",
      "1900-01-01 16:50:00.835000\n",
      "1900-01-01 17:03:59.344000\n",
      "1900-01-01 17:41:26.590000\n",
      "1900-01-01 17:48:23.778000\n",
      "1900-01-01 17:57:07.721000\n",
      "check2\n",
      "5\n",
      "5 długość start markers\n",
      "15 hjeolo\n",
      "942691\n",
      "50859\n",
      "21 hjeolo\n",
      "3193638\n",
      "51325\n",
      "25 hjeolo\n",
      "3592642\n",
      "50872\n",
      "35 hjeolo\n",
      "4131545\n",
      "50993\n",
      "check 3\n",
      "/home/ubuntu/eSportData/jupyter-data/akcelerometry/S1/S1_235.csv acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2862483/608676873.py:248: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  acc['time'] = pd.to_datetime(acc['time'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:13:52.001000\n",
      "18:00:52.517000\n",
      "main path\n",
      "0.0\n",
      "0.0\n",
      "check4\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def S1_processing(root, path_tx, path_acc, rej, output_errors, id):\n",
    "    \"\"\"\n",
    "    main pipeline\n",
    "    input:\n",
    "        root: Main path to vuamsa files\n",
    "        path_tx: path to beatscope files\n",
    "        path_acc: path to accelerometers\n",
    "        rej = empty list for people who for some reason were not processed\n",
    "    output:\n",
    "        processed databases and list of people who were not processed by the pipeline\n",
    "    \"\"\"\n",
    "    with open(output_errors, 'w') as error_log:\n",
    "                start_0 = None\n",
    "                start_1 = None\n",
    "                start_2 = None\n",
    "                start_3 = None\n",
    "                start_4 = None\n",
    "                try:\n",
    "                    \n",
    "                    bs_columns = ['timestamp', 'SBP', 'DBP', 'MBP', 'HR', 'SV', 'LVET', 'PI', 'MS', 'CO', 'TPR', 'TPRCGS', 'marker_tx', 'dummy']\n",
    "                    da_bs_columns = ['timestamp_x', 'ECG','DZ' ,'DZDT','Z0', 'marker', 'timestamp_y', 'SBP', 'DBP', 'MBP', 'HR', 'SV', 'LVET', 'PI', 'MS', 'CO', 'TPR', 'TPRCGS', 'marker_tx','dummy']\n",
    "                    col_names = ['timestamp','ECG', 'DZ', 'DZDT', 'Z0', 'marker']\n",
    "                    markers_keys = {'1':'m0', '15':'m1', '21':'m2', '25': 'm3', '35':'m4'} # vuams|beatscope marker dictionary for the first procedure saved as strings\n",
    "                    ids = str(id)  # identifier of the person whose files are being processed\n",
    "                    print(ids, '-person number')\n",
    "        \n",
    "                    file_names = []\n",
    "                    extended_txs = []\n",
    "                    \n",
    "                    for filename in os.listdir(root):  # list of all files from paths\n",
    "                        \n",
    "                        if os.path.isfile(os.path.join(root, filename)):\n",
    "                            file_names.append(filename)\n",
    "                    \n",
    "                    for filename in os.listdir(path_tx):\n",
    "                        \n",
    "                        if os.path.isfile(os.path.join(path_tx, filename)):\n",
    "                            file_names.append(filename)\n",
    "                    for filename in os.listdir(path_acc):\n",
    "                        \n",
    "                        if os.path.isfile(os.path.join(path_acc, filename)):\n",
    "                            file_names.append(filename)\n",
    "                    \n",
    "                    #   create a pattern based on the identifier to include only files with relevant numbers in the list    \n",
    "                    pattern = fr'(?<![0-9m]){re.escape(ids)}(?![0-9])'\n",
    "                    subject_id = [file_name for file_name in file_names if re.search(pattern, file_name)]\n",
    "                    # patterns for vuamsa and acc files from files selected for a given person based on id containing appropriate keywords from file names        \n",
    "                    pattern_DZ = r'DZ[^A-Za-z]'\n",
    "                    pattern_DZDT = r'DZDT'\n",
    "                    pattern_ECG = r'ECG'\n",
    "                    pattern_Z0 = r'Z0'\n",
    "                    pattern_acc = fr'_{re.escape(ids)}'\n",
    "                    \n",
    "                    # merging DA\n",
    "                    empty_rows_dz = pd.DataFrame({'index': [None] * 180000, 'DZ': [None] * 180000, 'marker1': [None] * 180000, 'marker2': [None] * 180000, 'dummy': [None] * 180000})\n",
    "                    empty_rows_dzdt = pd.DataFrame({'index': [None] * 180000, 'DZDT': [None] * 180000, 'marker1': [None] * 180000, 'marker2': [None] * 180000, 'dummy': [None] * 180000})\n",
    "                    empty_rows_ecg = pd.DataFrame({'index': [None] * 180000, 'ECG': [None] * 180000, 'marker1': [None] * 180000, 'marker2': [None] * 180000, 'dummy': [None] * 180000})\n",
    "                    empty_rows_z0 = pd.DataFrame({'index': [None] * 180000, 'Z0': [None] * 180000, 'marker1': [None] * 180000, 'marker2': [None] * 180000, 'dummy': [None] * 180000})\n",
    "\n",
    "                    for file_ in subject_id:\n",
    "                        # Creating dataframes from all vuamsa files\n",
    "                        if re.search(pattern_DZ, file_):\n",
    "                            file_path_dz = os.path.join('/home/ubuntu/eSportData/jupyter-data/VU_AMS/s1/', file_)\n",
    "                            df_dz = pd.read_csv(file_path_dz, sep=\" \", header=None, names=['index', 'DZ', 'marker1', 'marker2', 'dummy'], skiprows=3)\n",
    "                            df_dz = pd.concat([df_dz, empty_rows_dz], ignore_index=True)\n",
    "                    \n",
    "                        elif re.search(pattern_DZDT, file_):\n",
    "                            file_path_dzdt = os.path.join('/home/ubuntu/eSportData/jupyter-data/VU_AMS/s1/', file_)\n",
    "                            df_dzdt = pd.read_csv(file_path_dzdt, sep=\" \", header=None, names=['index', 'DZDT', 'marker1', 'marker2', 'dummy'], skiprows=3)\n",
    "                            df_dzdt = pd.concat([df_dzdt, empty_rows_dzdt], ignore_index=True)\n",
    "                    \n",
    "                        elif re.search(pattern_ECG, file_):\n",
    "                            file_path_ecg = os.path.join('/home/ubuntu/eSportData/jupyter-data/VU_AMS/s1/', file_)\n",
    "                            time = extract_time_from_file(file_path_ecg)  # reading time from the ECG file using a function\n",
    "                            print(time)\n",
    "                            df_ecg = pd.read_csv(file_path_ecg, sep=\" \", header=None, names=['index', 'ECG', 'marker1', 'marker2', 'dummy'], skiprows=3)\n",
    "                            df_ecg = pd.concat([df_ecg, empty_rows_ecg], ignore_index=True)\n",
    "                    \n",
    "                        elif re.search(pattern_Z0, file_):\n",
    "                            file_path_z0 = os.path.join('/home/ubuntu/eSportData/jupyter-data/VU_AMS/s1/', file_)  # also extend Z0 to 1000ms from 250\n",
    "                            df_z0 = pd.read_csv(file_path_z0, sep=\" \", header=None, names=['index', 'Z0', 'marker1', 'marker2', 'dummy'], skiprows=3)\n",
    "                            df_z0 = pd.concat([pd.DataFrame({'index': [df_z0['index'].min() - 1], 'Z0': [df_z0['Z0'].iloc[0]], 'marker1': [df_z0['marker1'].iloc[0]]}), df_z0])\n",
    "                            new_index = range(df_z0['index'].min(), df_z0['index'].max() + 1)\n",
    "                            df_z0 = df_z0.set_index('index').reindex(new_index)\n",
    "                            df_z0 = df_z0.ffill()\n",
    "                            df_z0 = df_z0.reset_index()\n",
    "                            df_z0 = pd.concat([df_z0, empty_rows_z0], ignore_index=True)\n",
    "                        elif re.search(pattern_acc, file_):\n",
    "                            # acc_name = file_\n",
    "                            acc_name = file_\n",
    "                        \n",
    "                    \n",
    "                    ecg_col = df_ecg['ECG']\n",
    "                    dz_col = df_dz['DZ']\n",
    "                    dzdt_col = df_dzdt['DZDT']\n",
    "                    z0_col = df_z0['Z0']\n",
    "                    marker = df_ecg['marker2']\n",
    "                    min_length = len(df_z0)\n",
    "                    df_ecg = df_ecg.head(min_length)\n",
    "                    df_dz = df_dz.head(min_length)\n",
    "                    df_dzdt = df_dzdt.head(min_length)\n",
    "                    data_dict = {'ECG': ecg_col, 'DZ': dz_col, 'DZDT': dzdt_col, 'Z0': z0_col, 'marker':marker}\n",
    "                    da = pd.DataFrame(data_dict, index=df_dzdt.index, columns=col_names)\n",
    "                    da.iloc[0,0] = time # attached read time to an empty column in the first position\n",
    "                    da['timestamp'] = pd.to_datetime(da['timestamp'], format='%H:%M:%S.%f')\n",
    "\n",
    "                    first_valid_index = 0\n",
    "\n",
    "                    not_null_time = da.loc[first_valid_index, 'timestamp']\n",
    "                    da_list = da.values.tolist()\n",
    "                    for i in range(first_valid_index + 1, len(da_list)): # iterate through the entire length of the set attaching a time 1 ms greater than the previous one, starting from the attached initial time\n",
    "                        not_null_time += timedelta(milliseconds=1)       # the operation is performed on the list instead of df to be faster\n",
    "                       \n",
    "                        da_list[i][0] = not_null_time\n",
    "                    \n",
    "                    da_full = pd.DataFrame(da_list, columns =col_names)\n",
    "                    da_full['timestamp'] = da_full['timestamp'].dt.time\n",
    "                    pattern = fr'(?<![0-9m]){re.escape(ids)}(?![0-9])' # patterns for beatscope files\n",
    "                    txs = [tx for tx in subject_id if re.search(r'_m', tx)]\n",
    "                    txs.sort() # sort them in a list so that I can iterate freely through them later when I attach them to the vuams df    \n",
    "                    for tx in txs: # reading and extending to 1000ms immediately\n",
    "                       \n",
    "                        file_path_tx = os.path.join('/home/ubuntu/eSportData/jupyter-data/beatscope/Pliki/S1', tx)\n",
    "                        tx_df = pd.read_csv(file_path_tx, sep=';', header=None, skiprows=10, encoding='cp1250', decimal=',', names=bs_columns)\n",
    "                          \n",
    "                        tx_df['timestamp'] = pd.to_datetime(tx_df['timestamp'], format='%H:%M:%S,%f')\n",
    "                        # print(tx_df.timestamp[100])\n",
    "                         # tx_df = tx_df[~tx_df.duplicated('timestamp', keep='first')]\n",
    "    \n",
    "                        tx_df = tx_df.set_index('timestamp')\n",
    "                        tx_df = tx_df.resample('ms').ffill()\n",
    "                        tx_df = tx_df.reset_index()\n",
    "                        tx_df['timestamp'] = tx_df['timestamp'].dt.time\n",
    "                        extended_txs.append(tx_df)\n",
    "                            \n",
    "                        extended_txs[-1].iloc[extended_txs[-1].index.max(), -2] = 'End' # add \"end\" label for the last beatscope in the last row to know where to cut if necessary\n",
    "                    \n",
    "                    da_sync_idx = da_full.index[da_full['marker'] != -9999][0] - 240000 # marker occurrence location - 4 minutes, because for some reason for baseline the markers did not appear at the beginning but after 4 minutes\n",
    "                    da_sync_idx2 = da_full.index[da_full['marker'] != -9999][0] # marker start location (incorrect)\n",
    "                    for i in range(da_sync_idx, da_sync_idx2):\n",
    "                        da_full.iloc[i, 5] = 1  # fill the empty space where the 201 markers should be\n",
    "                    \n",
    "                    garry = extended_txs[0]\n",
    "                    tx_sync_idx = garry.index[garry['marker_tx'].notna()][0] # find the marker from the first beatscope\n",
    "                                \n",
    "                    assert garry.at[tx_sync_idx, 'marker_tx'].startswith('m')\n",
    "                                \n",
    "                    rows_diff = tx_sync_idx - da_sync_idx\n",
    "                    \n",
    "                    if rows_diff > 0:  # \n",
    "                        garry = garry.iloc[rows_diff:]\n",
    "                        garry = garry.reset_index(drop=True)\n",
    "                    else:\n",
    "                        da_full = da_full.iloc[-rows_diff:] # trim the vuams df so that the vuams marker appears in the same place as the beatscope marker\n",
    "                        da_full = da_full.reset_index(drop=True) \n",
    "                    garry = garry.rename(columns = {'timestamp': 'timestamp_y'})\n",
    "                    da_bc = da_full.merge(garry, how='left', left_index=True, right_index=True) # merge vuams with beatscope\n",
    "                    time_col = da_full['timestamp']  # extract the time column from vuams\n",
    "                    \n",
    "                    trials = [pd.merge(time_col, tx, on='timestamp', how='left') for tx in extended_txs] # new list with beatscopes, with attached times from vuams\n",
    "                    # print(trials[-1].iloc[-1], 'last time value of beatscope')\n",
    "                    txs_list = [trial.values.tolist() for trial in trials]\n",
    "                    # txs_list = [trial.values.tolist() for trial in trials[1:]]\n",
    "                    # tx_list = txs_list[0]\n",
    "                    da_bc_list = da_bc.values.tolist()\n",
    "                    start_0 = da_bc.index[da_bc['marker'] ==1][0]\n",
    "                    start_1 = da_bc.index[da_bc['marker'] ==15][0]\n",
    "                    start_2 = da_bc.index[da_bc['marker'] == 21][0]\n",
    "                    start_3 = da_bc.index[da_bc['marker'] == 25][0]\n",
    "                    start_4 = da_bc.index[da_bc['marker'] == 35][0]\n",
    "                    start_markers = []\n",
    "                    for counter in range(5):\n",
    "                        try:\n",
    "                            start_marker = locals()[f'start_{counter}']\n",
    "                            \n",
    "                            if start_marker is not None:\n",
    "                                start_markers.append(start_marker)\n",
    "                        except:\n",
    "                            pass\n",
    "                   \n",
    "                    df_tx = merge_vs_tx(start_markers, da_bc_list, txs_list, da_bs_columns, da_bc, trials, markers_keys) # assign a new dataframe from the merged vuams with beatscope function call\n",
    "                   \n",
    "                    try:\n",
    "                #processing accelerometers\n",
    "                        file_path_acc = os.path.join('/home/ubuntu/eSportData/jupyter-data/akcelerometry/S1', acc_name)\n",
    "                        acc = pd.read_csv(file_path_acc, header=None, sep=';', encoding='cp1250', decimal=',', names = ['time', 'wr', 'tl', 'tr', '1', '2', '3'], usecols = ['time', 'wr', 'tl', 'tr'], skiprows = 11)\n",
    "                        print(file_path_acc, 'acc')\n",
    "                        \n",
    "                        acc = acc.iloc[:-4]\n",
    "                        # extending acc from 1000ms\n",
    "                        acc['time'] = acc['time'].astype(str)\n",
    "                        acc['time'] = pd.to_datetime(acc['time'])\n",
    "                        \n",
    "                        acc['timestamp'] = acc['time'].apply(lambda x: x + timedelta(microseconds=1000))\n",
    "                        acc = acc.iloc[:-4]\n",
    "                        acc['timestamp'] = pd.to_datetime(acc['timestamp'], format='%H:%M:%S.%f')\n",
    "                        acc = acc.set_index('timestamp')\n",
    "                        acc = acc.resample('ms').ffill()\n",
    "                        acc = acc.reset_index()\n",
    "                        acc['timestamp'] = acc['timestamp'].dt.time\n",
    "                        acc = acc.drop(columns = ['time'])\n",
    "                        print(acc.iloc[-1,0])\n",
    "                        print(df_tx.iloc[-1,0])\n",
    "                        acc_sync_value = df_tx.iloc[0,0] # trim the acc df to the dimensions of the merged vuams and beatscope df\n",
    "                        acc_sync_idx = acc['timestamp'].index[acc['timestamp'] == acc_sync_value][0]\n",
    "                        print('main path')\n",
    "                                    # acc = acc.iloc[acc_sync_idx-60000:]\n",
    "                        try: # for situations where for some reason the acc lasted shorter than the vuams \n",
    "                            acc_last_value = df_tx.iloc[-1,0]\n",
    "                            acc_last_idx = acc['timestamp'].index[acc['timestamp'] == acc_last_value][0]\n",
    "                            acc = acc.iloc[acc_sync_idx: acc_last_idx]\n",
    "                            acc = acc.reset_index(drop=True)\n",
    "                        except:\n",
    "                            pass\n",
    "                        df_tx = df_tx.drop(columns = ['timestamp_y'], axis = 1)\n",
    "                        df_tx = df_tx.rename(columns={'timestamp_x': 'timestamp'})\n",
    "                        df_all = df_tx.merge(acc, how='left', on = 'timestamp') # merge acc with vuams and beatscope and have the whole dataset\n",
    "                        df_all['marker_y'] = df_all['marker']\n",
    "                        df_all = df_all.drop(columns = ['dummy','marker', 'marker_tx'], axis = 1) # drop all unnecessary columns and rename them\n",
    "                        df_all = df_all.rename(columns={'marker_y': 'marker'})\n",
    "                    except:\n",
    "                        print('alter path')\n",
    "                        df_tx['wr'] = -9999\n",
    "                        df_tx['tl'] = -9999\n",
    "                        df_tx['tr'] = -9999\n",
    "                        df_tx = df_tx.drop(columns = ['timestamp_y', 'dummy', 'marker_tx'], axis = 1)\n",
    "                        df_tx = df_tx.rename(columns={'timestamp_x': 'timestamp', 'marker':'marker_old'})\n",
    "                        df_tx['marker'] = df_tx['marker_old']\n",
    "                        df_tx = df_tx.drop(columns = ['marker_old'], axis = 1)\n",
    "                        df_all = df_tx\n",
    " \n",
    "                    df_all['marker'] = df_all['marker'].fillna(-9999)\n",
    "                    df_all = df_all.iloc[:-2]\n",
    "                    print(df_all.iloc[0, -2])\n",
    "                    print(df_all.iloc[-1, -2])    \n",
    "                    # save = f'S2_p{ids}.csv'\n",
    "                    print('file completed')\n",
    "                    \n",
    "                    df_all.to_csv(f'/home/ubuntu/eSportData/jupyter-data/VU_AMS/s1_output/S1_p{ids}.csv', index=False) # save using id\n",
    "    \n",
    "                \n",
    "            # if a person cannot be processed, add id to the rejected list\n",
    "    \n",
    "                except Exception as e:\n",
    "                    error_message = f\"Error processing ID {ids}: {str(e)}\"\n",
    "                    print(error_message, file=error_log)\n",
    "\n",
    "    return output_errors\n",
    "repair = [232, 235] \n",
    "for i in repair:\n",
    "    error_file = S1_processing(path, path_tx, path_acc, rejected, output_errors_path, i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc446dd-2af2-4039-9a53-962183f7fb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "%system free -m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
